# AI Safety Mini Talk

## AI Safety
What it isn't about: AI becoming sentient, SkyNet etc.

Two main concepts:

Orthogonality thesis.
- Agent can have any combination of intelligence level & final goals.

Instrumental convergence
- Given any final goal, there is a set of commonly occurring sub-goals 

## Specification gaming

We can get a flavour of the implications of these concepts.
Particularly in the form of specification gaming:
- You're a game playing agent and are going to lose? Crash the game.
- Boat race? Just do infinite donuts to collect points [YT](https://youtu.be/nKJlF-olKmg?t=242).
- Hardware design (don't build a clock, build a receiver)
- C.f. arguments around divisive content on social etc
- [Examples of specification gaming](https://docs.google.com/spreadsheets/d/e/2PACX-1vRPiprOaC3HsCf5Tuum8bRfzYUiKLRqJmbOoC-32JorNdfyTiRRsR7Ea5eWtvsWzuxo8bjOxCG84dAg/pubhtml)

## References

Selection to give an idea of breadth:
- Rohin Shah's [Alignment Newsletter](https://rohinshah.com/alignment-newsletter/).
- Rob Miles's [AI Safety Channel](https://www.youtube.com/playlist?list=PLqL14ZxTTA4fyhYg6xD6Fz05WcuxLGseL)
- [stampy.ai](https://stampy.ai) - wiki / encyclopaedia for topics
- Nick Bostrom - *[Superintelligence](https://www.amazon.co.uk/Superintelligence-Dangers-Strategies-Nick-Bostrom/dp/0199678111)*
- [FTX Future fund]([https://ftxfuturefund.org/](https://ftxfuturefund.org/)) - funding for projects
