# AI Safety Mini Talk

### AI Safety
What it isn't about: AI becoming sentient, SkyNet etc.
Orthogonality thesis.
- Agent can have any combination of intelligence level & final goals.
Instrumental convergence
- Given any final goal, there is a set of commonly occurring sub-goals 

#### Some problems:
Interpretability
- Hard to understand what is going on inside.
Specification gaming
- Going to loose? Crash the game.
- Going round in circles, boat race.
- Facebook - divisive content etc
- Hardware design (don't build a clock, build a receiver)

#### Reference
Rob Miles - [AI Safety Channel](https://www.youtube.com/playlist?list=PLqL14ZxTTA4fyhYg6xD6Fz05WcuxLGseL)
[stampy.ai](https://stampy.ai) - wiki / encyclopaedia for topics
Nick Bostrom - [Superintelligence](https://www.amazon.co.uk/Superintelligence-Dangers-Strategies-Nick-Bostrom/dp/0199678111)
[FTX Future fund]([https://ftxfuturefund.org/](https://ftxfuturefund.org/)) - funding for projects
